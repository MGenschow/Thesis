{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../hyperstyle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8,\n",
      " 'board_interval': 50,\n",
      " 'checkpoint_path': None,\n",
      " 'dataset_type': 'zalando_germany_encode',\n",
      " 'encoder_type': 'SharedWeightsHyperNetResNet',\n",
      " 'exp_dir': '/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/hyperstyle/setup_test/',\n",
      " 'id_lambda': 0.0,\n",
      " 'image_interval': 100,\n",
      " 'input_nc': 6,\n",
      " 'l2_lambda': 1.0,\n",
      " 'layers_to_tune': '0,2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24',\n",
      " 'learning_rate': 0.0001,\n",
      " 'load_w_encoder': True,\n",
      " 'lpips_lambda': 0.8,\n",
      " 'max_steps': 500000,\n",
      " 'max_val_batches': 150,\n",
      " 'moco_lambda': 0.5,\n",
      " 'n_iters_per_batch': 5,\n",
      " 'optim_name': 'ranger',\n",
      " 'output_size': 512,\n",
      " 'save_interval': 500,\n",
      " 'stylegan_weights': '/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/hyperstyle/pretrained/00005-stylegan2_ada_images-mirror-auto2-kimg5000-resumeffhq512_network-snapshot-001200.pt',\n",
      " 'test_batch_size': 8,\n",
      " 'test_workers': 8,\n",
      " 'train_decoder': False,\n",
      " 'val_interval': 500,\n",
      " 'w_encoder_checkpoint_path': '/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/e4e/00005_snapshot_1200/setup/checkpoints/best_model.pt',\n",
      " 'w_encoder_type': 'WEncoder',\n",
      " 'workers': 8}\n",
      "Loading hypernet weights from resnet34!\n",
      "Loading decoder weights from pretrained path: /pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/hyperstyle/pretrained/00005-stylegan2_ada_images-mirror-auto2-kimg5000-resumeffhq512_network-snapshot-001200.pt\n",
      "Loading pretrained W encoder...\n",
      "Using WEncoder\n",
      "Loading WEncoder from checkpoint: /pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/e4e/00005_snapshot_1200/setup/checkpoints/best_model.pt\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/train.py\", line 32, in <module>\n",
      "    main()\n",
      "  File \"scripts/train.py\", line 19, in main\n",
      "    coach = Coach(opts)\n",
      "  File \"./training/coach_hyperstyle.py\", line 35, in __init__\n",
      "    self.net = HyperStyle(self.opts).to(self.device)\n",
      "  File \"./models/hyperstyle.py\", line 25, in __init__\n",
      "    self.load_weights()\n",
      "  File \"./models/hyperstyle.py\", line 61, in load_weights\n",
      "    self.w_encoder = self.__get_pretrained_w_encoder()\n",
      "  File \"./models/hyperstyle.py\", line 145, in __get_pretrained_w_encoder\n",
      "    w_net = pSp(opts_w_encoder)\n",
      "  File \"./models/encoders/psp.py\", line 33, in __init__\n",
      "    self.load_weights()\n",
      "  File \"./models/encoders/psp.py\", line 46, in load_weights\n",
      "    self.encoder.load_state_dict(get_keys(ckpt, 'encoder'), strict=True)\n",
      "  File \"/pfs/work7/workspace/scratch/tu_zxmav84-thesis/miniconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1052, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for WEncoder:\n",
      "\tMissing key(s) in state_dict: \"linear.weight\", \"linear.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"styles.0.convs.0.weight\", \"styles.0.convs.0.bias\", \"styles.0.convs.2.weight\", \"styles.0.convs.2.bias\", \"styles.0.convs.4.weight\", \"styles.0.convs.4.bias\", \"styles.0.convs.6.weight\", \"styles.0.convs.6.bias\", \"styles.0.linear.weight\", \"styles.0.linear.bias\", \"styles.1.convs.0.weight\", \"styles.1.convs.0.bias\", \"styles.1.convs.2.weight\", \"styles.1.convs.2.bias\", \"styles.1.convs.4.weight\", \"styles.1.convs.4.bias\", \"styles.1.convs.6.weight\", \"styles.1.convs.6.bias\", \"styles.1.linear.weight\", \"styles.1.linear.bias\", \"styles.2.convs.0.weight\", \"styles.2.convs.0.bias\", \"styles.2.convs.2.weight\", \"styles.2.convs.2.bias\", \"styles.2.convs.4.weight\", \"styles.2.convs.4.bias\", \"styles.2.convs.6.weight\", \"styles.2.convs.6.bias\", \"styles.2.linear.weight\", \"styles.2.linear.bias\", \"styles.3.convs.0.weight\", \"styles.3.convs.0.bias\", \"styles.3.convs.2.weight\", \"styles.3.convs.2.bias\", \"styles.3.convs.4.weight\", \"styles.3.convs.4.bias\", \"styles.3.convs.6.weight\", \"styles.3.convs.6.bias\", \"styles.3.convs.8.weight\", \"styles.3.convs.8.bias\", \"styles.3.linear.weight\", \"styles.3.linear.bias\", \"styles.4.convs.0.weight\", \"styles.4.convs.0.bias\", \"styles.4.convs.2.weight\", \"styles.4.convs.2.bias\", \"styles.4.convs.4.weight\", \"styles.4.convs.4.bias\", \"styles.4.convs.6.weight\", \"styles.4.convs.6.bias\", \"styles.4.convs.8.weight\", \"styles.4.convs.8.bias\", \"styles.4.linear.weight\", \"styles.4.linear.bias\", \"styles.5.convs.0.weight\", \"styles.5.convs.0.bias\", \"styles.5.convs.2.weight\", \"styles.5.convs.2.bias\", \"styles.5.convs.4.weight\", \"styles.5.convs.4.bias\", \"styles.5.convs.6.weight\", \"styles.5.convs.6.bias\", \"styles.5.convs.8.weight\", \"styles.5.convs.8.bias\", \"styles.5.linear.weight\", \"styles.5.linear.bias\", \"styles.6.convs.0.weight\", \"styles.6.convs.0.bias\", \"styles.6.convs.2.weight\", \"styles.6.convs.2.bias\", \"styles.6.convs.4.weight\", \"styles.6.convs.4.bias\", \"styles.6.convs.6.weight\", \"styles.6.convs.6.bias\", \"styles.6.convs.8.weight\", \"styles.6.convs.8.bias\", \"styles.6.linear.weight\", \"styles.6.linear.bias\", \"styles.7.convs.0.weight\", \"styles.7.convs.0.bias\", \"styles.7.convs.2.weight\", \"styles.7.convs.2.bias\", \"styles.7.convs.4.weight\", \"styles.7.convs.4.bias\", \"styles.7.convs.6.weight\", \"styles.7.convs.6.bias\", \"styles.7.convs.8.weight\", \"styles.7.convs.8.bias\", \"styles.7.convs.10.weight\", \"styles.7.convs.10.bias\", \"styles.7.linear.weight\", \"styles.7.linear.bias\", \"styles.8.convs.0.weight\", \"styles.8.convs.0.bias\", \"styles.8.convs.2.weight\", \"styles.8.convs.2.bias\", \"styles.8.convs.4.weight\", \"styles.8.convs.4.bias\", \"styles.8.convs.6.weight\", \"styles.8.convs.6.bias\", \"styles.8.convs.8.weight\", \"styles.8.convs.8.bias\", \"styles.8.convs.10.weight\", \"styles.8.convs.10.bias\", \"styles.8.linear.weight\", \"styles.8.linear.bias\", \"styles.9.convs.0.weight\", \"styles.9.convs.0.bias\", \"styles.9.convs.2.weight\", \"styles.9.convs.2.bias\", \"styles.9.convs.4.weight\", \"styles.9.convs.4.bias\", \"styles.9.convs.6.weight\", \"styles.9.convs.6.bias\", \"styles.9.convs.8.weight\", \"styles.9.convs.8.bias\", \"styles.9.convs.10.weight\", \"styles.9.convs.10.bias\", \"styles.9.linear.weight\", \"styles.9.linear.bias\", \"styles.10.convs.0.weight\", \"styles.10.convs.0.bias\", \"styles.10.convs.2.weight\", \"styles.10.convs.2.bias\", \"styles.10.convs.4.weight\", \"styles.10.convs.4.bias\", \"styles.10.convs.6.weight\", \"styles.10.convs.6.bias\", \"styles.10.convs.8.weight\", \"styles.10.convs.8.bias\", \"styles.10.convs.10.weight\", \"styles.10.convs.10.bias\", \"styles.10.linear.weight\", \"styles.10.linear.bias\", \"styles.11.convs.0.weight\", \"styles.11.convs.0.bias\", \"styles.11.convs.2.weight\", \"styles.11.convs.2.bias\", \"styles.11.convs.4.weight\", \"styles.11.convs.4.bias\", \"styles.11.convs.6.weight\", \"styles.11.convs.6.bias\", \"styles.11.convs.8.weight\", \"styles.11.convs.8.bias\", \"styles.11.convs.10.weight\", \"styles.11.convs.10.bias\", \"styles.11.linear.weight\", \"styles.11.linear.bias\", \"styles.12.convs.0.weight\", \"styles.12.convs.0.bias\", \"styles.12.convs.2.weight\", \"styles.12.convs.2.bias\", \"styles.12.convs.4.weight\", \"styles.12.convs.4.bias\", \"styles.12.convs.6.weight\", \"styles.12.convs.6.bias\", \"styles.12.convs.8.weight\", \"styles.12.convs.8.bias\", \"styles.12.convs.10.weight\", \"styles.12.convs.10.bias\", \"styles.12.linear.weight\", \"styles.12.linear.bias\", \"styles.13.convs.0.weight\", \"styles.13.convs.0.bias\", \"styles.13.convs.2.weight\", \"styles.13.convs.2.bias\", \"styles.13.convs.4.weight\", \"styles.13.convs.4.bias\", \"styles.13.convs.6.weight\", \"styles.13.convs.6.bias\", \"styles.13.convs.8.weight\", \"styles.13.convs.8.bias\", \"styles.13.convs.10.weight\", \"styles.13.convs.10.bias\", \"styles.13.linear.weight\", \"styles.13.linear.bias\", \"styles.14.convs.0.weight\", \"styles.14.convs.0.bias\", \"styles.14.convs.2.weight\", \"styles.14.convs.2.bias\", \"styles.14.convs.4.weight\", \"styles.14.convs.4.bias\", \"styles.14.convs.6.weight\", \"styles.14.convs.6.bias\", \"styles.14.convs.8.weight\", \"styles.14.convs.8.bias\", \"styles.14.convs.10.weight\", \"styles.14.convs.10.bias\", \"styles.14.linear.weight\", \"styles.14.linear.bias\", \"styles.15.convs.0.weight\", \"styles.15.convs.0.bias\", \"styles.15.convs.2.weight\", \"styles.15.convs.2.bias\", \"styles.15.convs.4.weight\", \"styles.15.convs.4.bias\", \"styles.15.convs.6.weight\", \"styles.15.convs.6.bias\", \"styles.15.convs.8.weight\", \"styles.15.convs.8.bias\", \"styles.15.convs.10.weight\", \"styles.15.convs.10.bias\", \"styles.15.linear.weight\", \"styles.15.linear.bias\", \"latlayer1.weight\", \"latlayer1.bias\", \"latlayer2.weight\", \"latlayer2.bias\". \n"
     ]
    }
   ],
   "source": [
    "CMD = \"\"\" python scripts/train.py \\\n",
    "    --dataset_type=zalando_germany_encode \\\n",
    "    --encoder_type=SharedWeightsHyperNetResNet \\\n",
    "    --exp_dir=/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/hyperstyle/setup_test/ \\\n",
    "    --stylegan_weights /pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/hyperstyle/pretrained/00005-stylegan2_ada_images-mirror-auto2-kimg5000-resumeffhq512_network-snapshot-001200.pt \\\n",
    "    --workers=8 \\\n",
    "    --batch_size=8 \\\n",
    "    --test_batch_size=8 \\\n",
    "    --test_workers=8 \\\n",
    "    --val_interval=500 \\\n",
    "    --save_interval=500 \\\n",
    "    --lpips_lambda=0.8 \\\n",
    "    --l2_lambda=1 \\\n",
    "    --id_lambda=0 \\\n",
    "    --moco_lambda=0.5 \\\n",
    "    --n_iters_per_batch=5 \\\n",
    "    --max_val_batches=150 \\\n",
    "    --output_size=512 \\\n",
    "    --load_w_encoder \\\n",
    "    --w_encoder_checkpoint_path=/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync/Models/e4e/00005_snapshot_1200/setup/checkpoints/best_model.pt \\\n",
    "\"\"\"\n",
    "\n",
    "!{CMD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
