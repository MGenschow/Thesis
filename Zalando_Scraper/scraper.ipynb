{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_token import *\n",
    "import json\n",
    "from apify_client import ApifyClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Data.nosync/\" # Relative path to the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Subcategories\n",
    "- In order to run the Zalando Scraper from Apify, one should have the urls of the various sub-categories listed on Zalando. \n",
    "- Of course, one could also fetch everything from one category (e.g. dresses) directly. \n",
    "- The problem with this approach is that the number of items to be scraped is very high and the scraper might not be able to handle it. \n",
    "- Therefore, it is recommended to scrape sub-categories separately. \n",
    "\n",
    "\n",
    "- The code below first retrieves the names and urls of the categoreis from Zalando and then retrieves the sub-categories of each category. \n",
    "- Subcategoreis for the category dress are e.g. \"cocktail dress\", \"maxi dress\", \"shirt dress\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url_content(url):\n",
    "    # Use the requests library to fetch the content of the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch content. HTTP status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subcategories(cat_url):\n",
    "    response = fetch_url_content(cat_url)\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "    subcats = soup.find('ul', class_ = 'ODGSbs').find('ul', class_ = 'ODGSbs')\n",
    "    subcats = subcats.find_all('li')\n",
    "    subcats = {elem.text: elem.find('a')['href'] for elem in subcats}\n",
    "    return subcats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories_dict.json exists; skipping fetching categories\n",
      "Categories loaded from categories_dict.json\n"
     ]
    }
   ],
   "source": [
    "# Only run this code if the categories dict does not exist already\n",
    "if not os.path.exists('categories_dict.json'):\n",
    "    print(\"Fetching categories\")\n",
    "\n",
    "    # Get the toplevel categories from the Zalando website\n",
    "    response = fetch_url_content(\"https://en.zalando.de/womens-clothing/\")\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "    subcats = soup.find('ul', class_ = 'ODGSbs')\n",
    "    subcats = subcats.find_all('li')\n",
    "    garment_categories = {elem.text: elem.find('a')['href'] for elem in subcats}\n",
    "\n",
    "    # Remove the 'Sale' category since this is very different from the other categories\n",
    "    if 'Sale' in garment_categories:\n",
    "        del garment_categories['Sale']\n",
    "\n",
    "    # Get the subcategories for each toplevel category\n",
    "    all_categories = {}\n",
    "    for cat_name, cat_url in tqdm(garment_categories.items(), desc='Fetching sub-categories'):\n",
    "        try:\n",
    "            subcats = get_subcategories(cat_url)\n",
    "        except:\n",
    "            print(f\"Failed to fetch subcategories for {cat_name}\")\n",
    "            subcats = {}\n",
    "        all_categories[cat_name] = {\n",
    "            'url': cat_url,\n",
    "            'subcategories': subcats\n",
    "        }\n",
    "        # Sleep for a random amount of time to avoid getting blocked\n",
    "        time.sleep(np.random.uniform(1, 5))\n",
    "\n",
    "    # Save the categories dict to a JSON file\n",
    "    with open('categories_dict.json', 'w') as f:\n",
    "        json.dump(all_categories, f)\n",
    "    print(\"Categories saved to categories_dict.json\")\n",
    "else:\n",
    "    # Load the categories dict from the JSON file\n",
    "    with open('categories_dict.json', 'r') as f:\n",
    "        all_categories = json.load(f)\n",
    "    print(\"categories_dict.json exists; skipping fetching categories\")\n",
    "    print(\"Categories loaded from categories_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape the Article Data from Zalando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zalando_scraper(start_url, out_name, max_items=None,):\n",
    "    # Initialize the ApifyClient with your API token\n",
    "    client = ApifyClient(API_KEY)\n",
    "\n",
    "    # Prepare the Actor input\n",
    "    run_input = {\n",
    "        \"startUrls\": [\n",
    "            start_url\n",
    "        ],\n",
    "        \"maxItems\": max_items,\n",
    "        \"proxy\": {\n",
    "            \"useApifyProxy\": True,\n",
    "            #\"apifyProxyCountry\": \"DE\",\n",
    "            \"apifyProxyGroups\": [\"RESIDENTIAL\"],\n",
    "        },\n",
    "        }\n",
    "    if max_items == None:\n",
    "        del run_input[\"maxItems\"]\n",
    "\n",
    "    # Run the Actor and wait for it to finish\n",
    "    run = client.actor(\"wPoILN4JczGRGC1xe\").call(run_input=run_input)\n",
    "\n",
    "    # Concat all items to list and dump to JSON\n",
    "    articles = []\n",
    "    for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "        articles.append(item)\n",
    "    \n",
    "    with open(out_name, 'w') as f:\n",
    "        json.dump(articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Dresses', 'T-shirts & tops', 'Trousers', 'Jeans', 'Shirts & Blouses', 'Jackets & Blazers', 'Swimwear', 'Sweatshirts & Hoodies', 'Skirts', 'Knitwear & Cardigans', 'Sportswear', 'Shorts', 'Jumpsuits', 'Coats', 'Underwear', 'Nightwear & Loungewear', 'Socks & Tights'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_whole_category(cat_name):\n",
    "    # Make sure the folder exists\n",
    "    folder_name = cat_name.replace('&', 'and').replace(' ', '_').replace('-', '_').lower()\n",
    "    folder_path = f\"{DATA_PATH}Zalando_Germany_Dataset/metadata_dicts/{folder_name}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Get subcategory URLs\n",
    "    subcat_urls = all_categories[cat_name]['subcategories']\n",
    "\n",
    "    # Scrape each subcategory\n",
    "    for subcat in subcat_urls.keys():\n",
    "        category_name_clean = subcat.replace('&', 'and').replace(' ', '_').replace('-', '_').lower()\n",
    "        out_path = f\"{folder_path}/{category_name_clean}.json\"\n",
    "\n",
    "        if not os.path.exists(out_path):\n",
    "            print(f\"{datetime.datetime.now()}: Scraping {subcat}\")\n",
    "            zalando_scraper(subcat_urls[subcat], out_path, max_items=None)\n",
    "            print(f\"{datetime.datetime.now()}: Finished scraping {subcat}\")\n",
    "        else:\n",
    "            print(f\"Skipping {subcat} since it already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Casual Dresses since it already exists\n",
      "Skipping Evening Dresses since it already exists\n",
      "Skipping Occasion Dresses since it already exists\n",
      "Skipping Shirt Dresses since it already exists\n",
      "Skipping Jersey Dresses since it already exists\n",
      "Skipping Shift Dresses since it already exists\n",
      "Skipping Maxi Dresses since it already exists\n",
      "Skipping Denim Dresses since it already exists\n",
      "Skipping Knitted Dresses since it already exists\n",
      "Skipping Dirndl Dresses since it already exists\n"
     ]
    }
   ],
   "source": [
    "scrape_whole_category('Dresses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scrape the Packshot Images from Zalando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps: \n",
    "1. Read in metadata\n",
    "2. Loop over all articles\n",
    "3. If article HTML does not exist in the folder, request and save it there\n",
    "4. Load the HTML and parse using BeautifulSoup\n",
    "5. Save all image data in the image_dicts folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Gamrent Categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Metadata Files for selected Category:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/jersey_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/dirndl_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/shift_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/occasion_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/shirt_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/evening_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/knitted_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/casual_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/denim_dresses.json',\n",
       " '../../Data.nosync/Zalando_Germany_Dataset/metadata_dicts/dresses/maxi_dresses.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_metadata_folders = glob(f\"{DATA_PATH}Zalando_Germany_Dataset/metadata_dicts/*/\")\n",
    "print(\"Existing Gamrent Categories:\")\n",
    "display(existing_metadata_folders)\n",
    "\n",
    "print(\"Existing Metadata Files for selected Category:\")\n",
    "folder_to_scrape = existing_metadata_folders[0]\n",
    "existing_metadata_files = glob(f\"{folder_to_scrape}*.json\")\n",
    "existing_metadata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_url_content(url, file_path):\n",
    "    # Use the requests library to fetch the content of the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Open a file for writing in binary mode (to accommodate all content types)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        #print(f\"Content saved to {file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch content. HTTP status code: {response.status_code}\") \n",
    "        # This will print if the request was unsuccessful so that the scraper can be stopped immediately. \n",
    "        # This might prevent you from being blocked by the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_packshot_link(html_content):\n",
    "    product_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # find list of images\n",
    "    product_img_thumbs = product_soup.find('ul', attrs={\"aria-label\": \"Product media gallery\"})\n",
    "\n",
    "    # in some page designs, the images are wrapped in a div\n",
    "    if product_img_thumbs is None:\n",
    "        product_img_thumbs = product_soup.find('div', attrs={\"class\": \"I7OI1O C3wGFf L5YdXz _0xLoFW _7ckuOK mROyo1 _5qdMrS\"})\n",
    "\n",
    "    # find all image objects\n",
    "    try:\n",
    "        thumb_links = product_img_thumbs.find_all('img')\n",
    "        frontal_img_link = None\n",
    "        for thumb in thumb_links:\n",
    "            # get images links from source code\n",
    "            thumb = re.findall(r'src=\".+?\"', str(thumb))\n",
    "            thumb = str(thumb)[7:-3]\n",
    "            # packshot signifies that this image shows the frontal view of the product with no model\n",
    "            if \"packshot\" in thumb:\n",
    "                replacement_res = \"500\"\n",
    "                # bring image link into the right format\n",
    "                frontal_img_link = re.sub(r'(?<=imwidth=).+?(?=&)', replacement_res, thumb, flags=re.S)\n",
    "    except:\n",
    "        frontal_img_link = None\n",
    "\n",
    "    return frontal_img_link     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_image_data(metadata_file):\n",
    "    # Create folder to save the image dicts\n",
    "    folder_to_save_images = metadata_file.replace('metadata_dicts', 'image_dicts')\n",
    "    folder_to_save_images = '/'.join(folder_to_save_images.split('/')[:-1])+ '/'\n",
    "    if not os.path.exists(folder_to_save_images):\n",
    "        os.makedirs(folder_to_save_images)\n",
    "    dict_save_path = f\"{folder_to_save_images}{metadata_file.split('/')[-1]}\"\n",
    "    if os.path.exists(dict_save_path):\n",
    "        print(f\"Skipping {metadata_file.split('/')[-1]} since it already exists\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    print(f\"Scraping images for {metadata_file.split('/')[-1]}\")\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # Create a folder to save the HTML files\n",
    "    folder_to_save = metadata_file.replace('metadata_dicts', 'html_files')\n",
    "    folder_to_save = '/'.join(folder_to_save.split('/')[:-1])+ '/'\n",
    "    if not os.path.exists(folder_to_save):\n",
    "        os.makedirs(folder_to_save)\n",
    "    else: \n",
    "        print(f\"Current number of files in folder: {len(os.listdir(folder_to_save))}\")\n",
    "\n",
    "    # Create dictionary with all image links\n",
    "    image_dict = {}\n",
    "\n",
    "    for article in tqdm(metadata, desc='Fetching images'):\n",
    "        # Fetch the URL and save the content\n",
    "        url = article['url']\n",
    "        save_path = f\"{folder_to_save}{article['sku']}.html\"\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            fetch_and_save_url_content(url, save_path)\n",
    "            time.sleep(np.random.uniform(0, 1))\n",
    "\n",
    "        # Load the HTML file\n",
    "        try:\n",
    "            with open(save_path, 'r') as f:\n",
    "                html_content = f.read()\n",
    "\n",
    "            # Extract the packshot link\n",
    "            packshot_link = extract_packshot_link(html_content)\n",
    "            image_dict[article['sku']] = {'images':article['images'], \n",
    "                                        'packshot_link':packshot_link, \n",
    "                                        'thumbnail':article['thumbnail']}\n",
    "        except:\n",
    "            image_dict[article['sku']] = {'images':article['images'], \n",
    "                                        'packshot_link':None, \n",
    "                                        'thumbnail':article['thumbnail']}\n",
    "\n",
    "        # Save the image dict to a JSON file\n",
    "    \n",
    "    with open(dict_save_path, 'w') as f:\n",
    "        json.dump(image_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping jersey_dresses.json since it already exists\n",
      "Skipping dirndl_dresses.json since it already exists\n",
      "Skipping shift_dresses.json since it already exists\n",
      "Skipping occasion_dresses.json since it already exists\n",
      "Skipping shirt_dresses.json since it already exists\n",
      "Skipping evening_dresses.json since it already exists\n",
      "Skipping knitted_dresses.json since it already exists\n",
      "Skipping casual_dresses.json since it already exists\n",
      "Skipping denim_dresses.json since it already exists\n",
      "Skipping maxi_dresses.json since it already exists\n"
     ]
    }
   ],
   "source": [
    "for metadata_file in existing_metadata_files:\n",
    "    scrape_image_data(metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to test current IP Adress in use: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public IP address is: 185.104.138.53\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    " \n",
    "def get_public_ip(): \n",
    "    try: \n",
    "        response = requests.get('https://httpbin.org/ip') \n",
    "        if response.status_code == 200: \n",
    "            ip_data = response.json() \n",
    "            public_ip = ip_data.get('origin') \n",
    "            return public_ip \n",
    "        else: \n",
    "            print(f\"Failed to retrieve IP (Status code: {response.status_code})\") \n",
    "    except Exception as e: \n",
    "        print(f\"Error: {e}\") \n",
    " \n",
    "# Get and print the public IP address \n",
    "public_ip = get_public_ip() \n",
    "print(f\"Your public IP address is: {public_ip}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
