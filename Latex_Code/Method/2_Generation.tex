% Generation 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generation}
While multiple GAN-based architectures might be suitable, StyleGAN-based architectures have become the golden standard for image editing \citep[p.1]{bermano2022state}. This is mainly because they learn a highly disentangled and smooth latent space which allows for semantic attribute manipulation (see section \ref{sec:gans}). Within the family of StyleGAN-based architectures, StyleGAN2-Ada \citep{stylegan2ada} is chosen due to its remarkable capabilities to be trained on a small sample. Since the dataset acquired for training is limited by the availability of products on Zalando, training a traditional GAN model is not easily possible. Using StyleGAN2-Ada allows to train a high-fidelity generator even with a dataset of only 14k images.

\subsubsection{Training} \label{sec:sg2_training}
The StyleGAN2-Ada model used in this work has been trained for a resolution of 512x512 pixels per image using the "auto" configuration as defined in \cite{stylegan2ada} and additional x-flip augmentations to increase the dataset size. This configuration uses a reduced depth of the mapping network from 8 to 2 linear layers as this is sufficient according to the authors \citep[Appendix p.35]{stylegan2}. A transfer learning approach from the FFHQ-dataset \citep{stylegan1} is chosen since both datasets rely on centered images with non-relevant backgrounds. Using a pre-trained StyleGAN2-Ada model as the starting point speeds up the training process. The model has been trained for 26 hours on two NVIDIA Tesla V100 GPUs with 32GB VRAM. In total, 2000k images have been shown to the discriminator. Network snapshots have been saved every 40k images shown to the discriminator to later determine the best snapshot using the evaluation metrics described in section \ref{sec:gan_evaluation}.

\subsubsection{Evaluation}\label{sec:gan_evaluation}
The two most widely used GAN evaluation metrics are the Fr\'echet Inception Distance (FID) and the Inception Score (IS) \citep[p.2]{borji2022pros}. The IS \citep{salimans2016improved} evaluates the quality of generated images using ImegNet \citep{deng2009imagenet} predictions from an InceptionV3 \citep{szegedy2016rethinking} model. It combines the confidence of the conditional class predictions (indicating image quality) with the diversity of the predicted classes across the dataset (indicating image diversity). FID \citep{heusel2017gans} on the other hand calculates features of the real and generated datasets using intermediate layers of the same InceptionV3 \citep{szegedy2016rethinking} network and computes the Fr\'echet Inception Distance distance between them. \cite{heusel2017gans} define the FID using multivariate Gaussians over the extracted features as: 
$$\text{FID}(x, g) = \| \bs{\mu_x} - \bs{\mu_g} \|^2 + \text{Tr}\left( \bs{\Sigma_x} + \bs{\Sigma_g} - 2(\bs{\Sigma_x}  \bs{\Sigma_g})^{1/2} \right),$$
where $\bs{\mu_x}$, $\bs{\mu_g}$, $\bs{\Sigma_x} $ and $\bs{\Sigma_g}$ are the mean vectors and covariance matrices of the real and generated image distributions, respectively, and $\text{Tr}$ denotes the trace of a matrix. 

FID is consistent with human inspection and can detect intra-class mode collapse \citep[p.2]{borji2022pros}. As a drawback, for FID to be accurate and comparable, it needs to be calculated on a large and comparable number of samples. Hence, in all my experiments, I calculate the FID based on 50k generated images. As an alternative method, \cite{binkowski2018demystifying} proposed the Kernel Inception Distance (KID) which promises to be an unbiased version of the FID. In practice, there is a very high Spearman rank-order correlation between FID and KID \citep[p.3]{kurach2018gan}. While both the IS and the FID are single-score metrics that evaluate the quality and diversity of generations together, \cite{sajjadi2018assessing} propose improved formulations of precision and recall as GAN evaluation metrics. Recall estimates the fraction of the training set that the generator can reproduce while precision denotes the fraction of generated images that are realistic \citep[p.5]{kynkaanniemi2019improved}. Thus, when training a GAN, one aims for high precision and recall such that the generated images are as realistic as possible while also being as diverse as possible. Naturally, there is a trade-off between the two since a model that can create more diverse generations spans a much larger latent space, thus possibly allowing latent codes from ill-defined regions of this latent space that exhibit less realistic generations. To get a holistic assessment of the generation quality, I calculate all the above-mentioned evaluation metrics and select the best model snapshot based on a comprehensive analysis of the metrics.