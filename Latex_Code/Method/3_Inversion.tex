%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Inversion 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inversion}\label{sec:method_gan_inversion}
After a thorough literature review of available inversion methods (see section \ref{sec:gan_inversion}), a set of suitable methods was selected. The baseline model chosen is e4e \citep{tov2021designing} from the encoder-based models. Additionally, three hybrid models are tested: PTI \citep{roich2022pivotal}, ReStyle \citep{alaluf2021restyle}, and Hyperstyle \citep{alaluf2022hyperstyle}. Among these, PTI promises the best inversion performance but with a high inference time. To reduce inference time, ReStyle and Hyperstyle were selected as they promise similar results to PTI at a fraction of the time.

% After conducting a thorough literature research on available inversion methods (see section \ref{sec:gan_inversion}), a set of suitable methods has been selected. Since optimization-based methods have a prohibitively slow inference time and low editability performance \citep[p.3]{tov2021designing}, no pure optimization-based method has been selected. As a baseline inversion method, encoder4editing (e4e) \citep{tov2021designing} has been selected. This method promises high reconstruction quality while being specifically optimized to produce highly editable latent codes. Additionally, e4e is an encoder-based method and thus has a very fast inference time which allows to obtain latent codes for the complete dataset in a reasonable amount of time. Furthermore, many other inversion methods rely on e4e inversions as initial latent codes in their optimization. Using e4e, inversions into $\mathcal{W}^+$-space are obtained. The authors claim that there is a trade-off between distortion and editability when inverting into $\mathcal{W}$ or $\mathcal{W}^+$-space. While inversion into $\mathcal{W}^+$ leads to better reconstructions, it may come at the cost of editability \citep[p.5]{tov2021designing}. Encoder4editing for designed to specifically mitigate this, therefore in this work, only inversions into $\mathcal{W}^+$-space are used. From the domain of hybrid models, PTI \citep{roich2022pivotal} has been selected due to its proven state-of-the-art inversion performance \citep[p.1]{roich2022pivotal}. PTI builds upon e4e and uses its latent code as a starting point. It then pivots the generator weights such that the reconstruction quality increases. Since the latent code of the inversion is still the simple e4e inversion, it still lies in the editable region of the latent space, but thanks to the slightly changed generator, achieves much better reconstruction. Since the generator weights have to be optimized for each sample individually, inference time is very high. Therefore, two additional models have been tested which promise similar inversion performance at a fraction of the inference time. ReStyle \citep{alaluf2021restyle} and Hyperstyle \citep{alaluf2022hyperstyle} are models that again build upon e4e. ReStyle learns iterative refinement steps of the original e4e latent code by learning residual weights. Hyperstyle on the other hand builds upon ReStyle latent codes and additionally learns hyper-network weights for the convolutional filters of the generator. While this also changes the generator weights as in PTI, the hyper-network is learned during training and does not need to be optimized for every sample. Thus, inference time is drastically reduced.

\subsubsection{Training}
For training of the e4e model, the dataset has been randomly split into 90\% training samples and 10\% test samples. The model was trained with default parameters as defined in \cite{tov2021designing} and checkpoints were saved every 2000 steps.  In total, the model has been trained for 38 hours on one GPU. As can be seen in the loss curves for the overall loss, the LPIPS loss, and the L2 loss in figure \ref{fig:e4e_loss_curves}, the model has converged within this time. ReStyle and Hyperstyle have been trained with default parameters. ReStyle has initially been trained for 35 hours on one GPU but had not converged yet. Therefore it was trained for another 20 hours, totaling 55 hours of training time. Hyperstyle was trained for 40 hours and an additional 30 hours until convergence, totaling 70 hours of training time on one GPU. For all models, NVIDIA Tesla V100 GPUs with 32GB of VRAM were used.




\subsubsection{Evaluation}
Evaluating GAN inversion involves multiple dimensions. Generally, evaluation metrics can be grouped into three dimensions: Faithfulness (i.e. reconstruction quality), photorealism (i.e. perceptual quality),  and editability of the latent code \citep[p.3]{xia2022gan}. \\
Faithfulness measures the quality of the reconstruction, i.e. how closely the generated image from the inversion resembles the real image. One of the early measures proposed was Peak signal-to-noise ratio (PSNR). Many works additionally consider the Structural Similarity Index (SSIM) \citep{wang2004image} which measures the similarity using three key features: Luminance, Contrast, and Structure \cite[p.6]{wang2004image}. It has been shown that SSIM more closely correlates with human perception than PSNR \cite[p.19]{sheikh2006statistical}. For my evaluation, I use the advanced version of SSIM, the Multi Scale Structural Similarity Index Measures (MSSSIM) proposed by \cite{wang2003multiscale}.  Furthermore, multiple pixel-wise distances (e.g. mean squared error, mean absolute error) have been proposed. Since close reconstruction of input images is important in this work, the pixel-wise $\mathcal{L}_2$ distance will be used as an additional measure of faithfulness. \\
Measuring the perceptual quality of images (i.e. photorealism) against a reference dataset is described in chapter \ref{sec:gan_evaluation} in more detail. Many works rely on the same measures to evaluate the quality of the inversion process. I use the FID \citep{heusel2017gans} to measure how close the generated images from the inversions resemble the distribution of the real data. Furthermore, I use Learned Perceptual Image Patch Similarity (LPIPS) proposed by \cite{zhang2018unreasonable} which measures similarity between two images using features extracted from a VGG model \citep{simonyan2014very} trained on ImageNet \citep{deng2009imagenet}. \\
While the first two dimensions can be directly measured using multiple metrics, the editability of the latent code from an inversion method is not easily tested. Evaluating the editability involves yet another model, the editing technique. Thus, the evaluation of this dimension is very closely connected to the evaluation of the manipulation technique described in section \ref{sec:manipulation}. If the manipulation is not successful, it is difficult to find out whether the latent code itself is not editable or whether the manipulation method for the attribute does not work. As a qualitative measure for editing capabilities of inversions, \cite{zhu2020improved} propose to evaluate the quality of images generated from interpolating between two latent codes. They argue that an inversion is good if the interpolated images are of high quality \citep[p.10]{zhu2020improved}. In general, preserving the original identity during edits and only editing localized attributes is of high importance. \cite{alaluf2021only} and \cite{richardson2021encoding} use facial recognition networks as proposed in \citep{deng2019arcface} to assess identity preservation via cosine similarity of facial identity representations. For non-facial data, using this technique is challenging since training identity recognition networks for other domains is difficult \citep[p.13]{bermano2022state}. As \cite{tov2021designing} point out, all popular quantitative metrics to evaluate editing performance contradict each other and often the human judgment \citep[p.7]{tov2021designing}. \\
Given the difficulty of evaluating the editing capabilities of inversion methods, I focus on quantitative metrics for faithfulness and photorealism when comparing the different inversion methods. To assess the editing capabilities of the latent codes, I rely on interpolation experiments as proposed by \citep{zhu2020improved} and eventually on the results obtained from the downstream manipulation method.