\subsection{GAN Inversion}\label{sec:gan_inversion}
While synthesizing arbitrary fake images from a pre-trained generator is straightforward, editing real images is a non-trivial task. For latent space editing techniques to be applied to real images, one must first find the corresponding latent code of the real image using a \textit{GAN Inversion} technique. Initially introduced by \cite{zhu2016generative}, the goal of GAN inversion is to find a latent code $w \in \mathcal{W}$ which most closely reconstructs the real image. Following \cite{bermano2022state}, inversion can be formally defined as: Given a pre-trained generator $G: \mathcal{W} \rightarrow \mathcal{X}$ and real image $x \in \mathcal{X}$,  GAN inversion aims to find $w \in \mathcal{W}$ such that 
\begin{equation}
\label{eq:gan_inversion}
    w^* =  \argmin_{w} \mathcal{L}(x, G(w)),
\end{equation}
where $\mathcal{L}$ is any reconstruction loss. Due to the non-convexity of the generator, this leads to a non-convex optimization problem. Approaches to solve this problem can be grouped into learning-/encoder-based, optimization-based, and hybrid methods \citep[p.6]{xia2022gan} and will be explained below. Note that inversion can be executed in any latent space ($\mathcal{Z}$, $\mathcal{W}$, etc.) of any GAN architecture. In the following, however, I will focus on the latent spaces of StyleGAN-based architectures. 

\subsubsection{Latent Spaces}
Choosing the latent space for the inversion is an important design choice since this space should allow a precise reconstruction while facilitating disentangled editing and other downstream tasks \citep[p.5]{xia2022gan}. For StyleGAN-based architectures, most inversion techniques use $\mathcal{Z}$, $\mathcal{W}$, $\mathcal{W^+}$, $\mathcal{S}$, $\mathcal{P_N}$ or $\mathcal{P_N^+}$. \\
The first latent space in StyleGAN is $\mathcal{Z}$-space which follows a normal distribution and thus has a closed-form definition. As the generator learns a mapping $\mathcal{Z} \rightarrow \mathcal{X}$, images from the GAN's latent space can easily be sampled from $\mathcal{Z}$ \citep[p.5]{bermano2022state}. By using the nonlinear mapping network $f: \mathcal{Z} \rightarrow \mathcal{W}$ of the StyleGAN architecture (see section \ref{sec:gans}), a distribution of $\mathcal{W}$ is learned, which better captures the distribution of the real data \citep[p.6]{stylegan1} and has better disentanglement than $\mathcal{Z}$ \citep[p.7]{shen2020interpreting}. \cite{abdal2019image2stylegan} propose an extension of $\mathcal{W}$, $\mathcal{W^+}$ to solve the limited expressiveness of $\mathcal{W}$ when representing real images in the inversion task. While in $\mathcal{W}$ space all rows of the 16x512-dimensional latent vector (for 512x512 output images) are identical, $\mathcal{W^+}$ allows for 16 different 512-dimensional style vectors that are inserted at different layers of the synthesis network. While allowing for more precise reconstructions due to higher expressiveness, using $\mathcal{W^+}$ can come at the expense of image quality, as operating in $\mathcal{W^+}$ makes regions in the latent space available, which are outside of the distribution the generator was trained on \citep[p.5]{bermano2022state}. As almost all latent space manipulation methods work with the more common $\mathcal{Z^{(+)}}$- and $\mathcal{W^{(+)}}$-spaces, inversion methods that use $\mathcal{S}$-space (\cite{wu2021stylespace}) or $\mathcal{P_N}$- and $\mathcal{P_N^+}$-space (\cite{zhu2020improved}) are not very relevant to this work. For a schematic overview of the StyleGAN latent spaces, see figure \ref{fig:latent_spaces} in the appendix. Note that in this work, inversion and manipulation in $\mathcal{Z}$, $\mathcal{W}$ and $\mathcal{W^+}$ have been performed and eventually it was decided to focus only on $\mathcal{W^+}$ due to its superior performance in the inversion process.


\subsubsection{Inversion Methods}
\textbf{Optimization-based} GAN inversion techniques (e.g. \cite{yeh2017semantic}, \cite{creswell2018inverting}, \cite{gu2020image}, \cite{bau2020semantic}, \cite{lipton2017precise}) derive the latent vector for a given image by solving equation \ref{eq:gan_inversion} on an instance level. While this achieves high reconstruction quality of the real input images, this comes at the cost of computational efficiency \citep[p.2]{alaluf2021restyle} and lower editability \citep[p.3]{tov2021designing}. Therefore, no optimization-based methods were implemented in this work.

%\subsubsection{Encoder-Based Methods}
\textbf{Encoder-based} GAN inversion approaches mostly train an encoder network $E(x, \theta_E)$ to obtain a mapping from the input image to its latent code. The objective function closely resembles this of an autoencoder pipeline where the decoder is the fixed generator for which the inversion network is obtained \citep[p.6]{xia2022gan}. The training objective can be defined following \cite{bermano2022state}, p.9 as: 
\begin{equation}
    \theta^*_E = \arg \min_{\theta_E} \sum_i \mathcal{L}(\bs x_i, G(E_{\theta_E}(\bs x_i))).
\end{equation}
There have been extensive efforts to encoder-based architectures since \cite{luo2017learning} first proposed an autoencoder-inspired approach (e.g. \cite{kim2021exploiting}, \cite{wang2022high}, \cite{shanyan2020collaborative}, \cite{pidhorskyi2020adversarial}, \cite{kang2021gan}, \cite{richardson2021encoding}, \cite{he2016deep}). The encoder model used in this work is the state-of-the-art encoder model encoder4editing (e4e), introduced by \cite{tov2021designing}. e4e was designed with a particular focus on editing capabilities of the inversion.
 By analyzing the trade-offs between distortion, perceptual quality, and editability, the authors can design an encoder that ensures editability while maintaining a high reconstruction quality. They find that editability and perceptual quality are high when an image is inverted close to $\mathcal{W}$ but lives in $\mathcal{W^+}$ to simultaneously achieve low distortion in the generation \citep[p.2]{tov2021designing}. This is achieved by adding multiple loss terms to the encoder. First, distortion is minimized by adding a similarity loss based on a ResNet-50 \citep{he2016deep} network, an $\mathcal{L}_2$ loss term, and a loss term based on LPIPS \citep{zhang2018unreasonable}. To maximize perceptual quality, a loss term is used to minimize the variation within the learned latent codes \citep[p.5]{tov2021designing}. Furthermore, a latent discriminator \citep{nitzan2020face} is employed to ensure proximity of the latent code to $\mathcal{W}$ \citep[p.5]{tov2021designing}. Encoder4editing poses an important basis for many inversion techniques. See figure \ref{fig:e4e} in the appendix for a schematic overview of the method. While encoder-based methods are considerably faster than optimization-based methods, their reconstructions are less accurate \citep[p.10]{bermano2022state}. To combine the fast inference time of encoders and the generally better performance of optimization models, several hybrid models have been proposed.


%\subsubsection{Hybrid Models}
\textbf{Hybrid models} obtain an initial latent code using an encoder and then further optimize this on a per-image level (e.g. \cite{zhu2016generative}, \cite{zhu2020domain}). Out of the universe of hybrid models, the most promising models based on the overall literature have been selected and tested in this work. These methods are explained below in more detail. \cite{alaluf2021restyle} propose \textbf{ReStyle}, an iterative approach, in which latent codes obtained from an e4e encoder are iteratively improved by learning residuals. Instead of obtaining the inversion from a single forward pass, the encoder is iteratively refined using a small number of steps that leverage a feedback algorithm between the predicted and the real image \citep[p.2]{alaluf2021restyle}. Each iteration computes a residual vector correcting the previous latent code, enhancing high-frequency details and complex structures.  This feedback mechanism allows intermediate reconstructions to better align with the target image, yielding lower LPIPS and MSE scores, while preserving semantic consistency and producing visually coherent results \citep[p.6]{alaluf2021restyle}. For a detailed schema of ReStyle see figure \ref{fig:restyle} in the appendix.

\cite{alaluf2022hyperstyle} propose \textbf{HyperStyle} which uses a hypernetwork-based encoder that learns offsets for the convolutional filters of the generator alongside the initial latent codes that are obtained from a fixed encoder (e.g. ReStyle). Thus, inference time is by a magnitude lower than for instance-level optimization while promising comparable reconstruction results \citep[p.5]{alaluf2022hyperstyle}. HyperStyle claims to bridge the gap between high-quality reconstructions that are obtained from generator-optimization approaches and fast inference time in encoder-based methods. Furthermore, the iterative refinement scheme introduced in ReStyle \citep{alaluf2021restyle} is adapted to the hyper-network training to achieve stronger expressive power and more accurate inversions \citep[p.5]{alaluf2022hyperstyle}. For a detailed schema of HyperStyle see figure \ref{fig:hyperstyle} in the appendix.

\cite{roich2022pivotal} propose \textbf{PTI}, a novel approach that combines initial inversions from an encoder and subsequent pivotal tuning of the generator, which achieved state-of-the-art performance in both distortion and editability. The initial latent code is obtained from an e4e model and thus lies within the well-behaved region of the latent space which allows editability \citep[p.2]{tov2021designing}. This code is used as a pivot around which the generator is trained for a few steps to better reconstruct the image while keeping nearby identities intact through a regularization term \citep[p.1]{roich2022pivotal}. While this achieves very high reconstruction quality and editability, it is also computationally expensive and complicates downstream tasks because each inversion needs its fine-tuned generator weights. 

