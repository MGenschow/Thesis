\subsection{Architecture Selection}
In the landscape of deep generative modeling, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion models are the most commonly used architectures. While each of them has its strengths and weaknesses, I opt for the use of GANs. The main reasons for this will be described in this section. Since the goal of this thesis is to achieve highly controllable manipulations of visual attributes, having a well-defined, controllable, and interpolable latent space is a key requirement for the architecture. While both VAEs and GANs have such a latent space, VAEs lack the power to produce high-quality reconstructions \citep[p.2]{muller2024disentangling} and clear images \citep[p.1]{wang2020state}. Diffusion models on the other hand can produce high-fidelity images but have a highly unstructured latent space that lacks semantic meaning. Naively interpolating the latent space, as it is possible for GANs, leads to random and abrupt flickering and drastic changes in the generated image from diffusion models \citep[p.2]{zhang2024diffmorpher}. Making the latent space of diffusion models interpolable requires extensive efforts as e.g. \cite{zhang2024diffmorpher} has shown by interpolating between the LoRA \citep{hu2021lora} adaptions of two images. Another approach would be to interpolate between encodings of text prompts obtained using textual inversion in text-guided diffusion models like \cite{wang2023interpolating} have done. In general, a controllable and interpolable latent space with effortless learning of semantic meaning is possible only using GAN architectures. Furthermore, although GANs generally achieve less mode coverage (i.e. diversity) in their generations than VAEs and diffusion models \citep[p.1]{xiao2021tackling}, this does not pose a problem, since the application in this work builds on the narrow distribution of packshot images of dresses.