{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers_training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == 'Darwin':\n",
    "    DATA_PATH = \"/Users/maltegenschow/Documents/Uni/Thesis/Data.nosync\"\n",
    "elif platform.system() == 'Linux':\n",
    "    DATA_PATH = \"/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'category'\n",
    "retrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_json(f\"{DATA_PATH}/Zalando_Germany_Dataset/dresses/metadata/dresses_metadata.json\").T.reset_index().rename(columns={'index': 'sku'})\n",
    "#df = df.sample(4000)\n",
    "df, id2label, label2id = prepare_data(df, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    #ransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset, test_dataset = get_datasets(df, \n",
    "                f\"{DATA_PATH}/Zalando_Germany_Dataset/dresses/images/square_images/\", \n",
    "                train_transform, \n",
    "                test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(num_labels, frozen_backbone=True):\n",
    "    weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "    model = resnet50(weights=weights)\n",
    "\n",
    "    if frozen_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Change last layer: \n",
    "    num_labels = len(id2label)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_labels)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7834d65d6c4762a10d157bfe86f93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: LOSS within report interval: 1.9085091829299927 | ACCURACY within report interval: 0.325\n",
      "Batch 20: LOSS within report interval: 1.789617669582367 | ACCURACY within report interval: 0.365625\n",
      "Batch 30: LOSS within report interval: 1.71024329662323 | ACCURACY within report interval: 0.36875\n",
      "Batch 40: LOSS within report interval: 1.617199444770813 | ACCURACY within report interval: 0.375\n",
      "Batch 50: LOSS within report interval: 1.695075273513794 | ACCURACY within report interval: 0.385\n",
      "Batch 60: LOSS within report interval: 1.723379945755005 | ACCURACY within report interval: 0.38125\n",
      "Batch 70: LOSS within report interval: 1.7763778448104859 | ACCURACY within report interval: 0.37857142857142856\n",
      "Batch 80: LOSS within report interval: 1.6570648789405822 | ACCURACY within report interval: 0.38828125\n",
      "Batch 90: LOSS within report interval: 1.6552448511123656 | ACCURACY within report interval: 0.3875\n",
      "Batch 100: LOSS within report interval: 1.6525492429733277 | ACCURACY within report interval: 0.390625\n",
      "Batch 110: LOSS within report interval: 1.6108964562416077 | ACCURACY within report interval: 0.39147727272727273\n",
      "Batch 120: LOSS within report interval: 1.6551068544387817 | ACCURACY within report interval: 0.3932291666666667\n",
      "Batch 130: LOSS within report interval: 1.707271361351013 | ACCURACY within report interval: 0.39134615384615384\n",
      "Batch 140: LOSS within report interval: 1.5977715492248534 | ACCURACY within report interval: 0.39107142857142857\n",
      "Batch 150: LOSS within report interval: 1.6212531089782716 | ACCURACY within report interval: 0.39\n",
      "Batch 160: LOSS within report interval: 1.737122642993927 | ACCURACY within report interval: 0.3859375\n",
      "Batch 170: LOSS within report interval: 1.6548737287521362 | ACCURACY within report interval: 0.3863970588235294\n",
      "Batch 180: LOSS within report interval: 1.6454056024551391 | ACCURACY within report interval: 0.3871527777777778\n",
      "Batch 190: LOSS within report interval: 1.487240445613861 | ACCURACY within report interval: 0.39144736842105265\n",
      "Batch 200: LOSS within report interval: 1.5021313905715943 | ACCURACY within report interval: 0.3946875\n",
      "Batch 210: LOSS within report interval: 1.4532167434692382 | ACCURACY within report interval: 0.399702380952381\n",
      "Batch 220: LOSS within report interval: 1.5035354614257812 | ACCURACY within report interval: 0.4017045454545455\n",
      "Batch 230: LOSS within report interval: 1.5031005859375 | ACCURACY within report interval: 0.4027173913043478\n",
      "Batch 240: LOSS within report interval: 1.6294762372970581 | ACCURACY within report interval: 0.40260416666666665\n",
      "Batch 250: LOSS within report interval: 1.4775247931480409 | ACCURACY within report interval: 0.40525\n",
      "Batch 260: LOSS within report interval: 1.5723722457885743 | ACCURACY within report interval: 0.40360576923076924\n",
      "Batch 270: LOSS within report interval: 1.5775467157363892 | ACCURACY within report interval: 0.40162037037037035\n",
      "Batch 280: LOSS within report interval: 1.5046345591545105 | ACCURACY within report interval: 0.40379464285714284\n",
      "Batch 290: LOSS within report interval: 1.6502167105674743 | ACCURACY within report interval: 0.40301724137931033\n",
      "Batch 300: LOSS within report interval: 1.54248765707016 | ACCURACY within report interval: 0.40270833333333333\n",
      "Batch 310: LOSS within report interval: 1.4937183260917664 | ACCURACY within report interval: 0.4056451612903226\n",
      "Batch 320: LOSS within report interval: 1.4875089526176453 | ACCURACY within report interval: 0.4068359375\n",
      "Batch 330: LOSS within report interval: 1.534868884086609 | ACCURACY within report interval: 0.40738636363636366\n",
      "Batch 340: LOSS within report interval: 1.520615017414093 | ACCURACY within report interval: 0.4091911764705882\n",
      "Batch 350: LOSS within report interval: 1.6032874941825868 | ACCURACY within report interval: 0.4082142857142857\n",
      "Batch 360: LOSS within report interval: 1.3839582085609436 | ACCURACY within report interval: 0.41180555555555554\n",
      "Batch 370: LOSS within report interval: 1.404383909702301 | ACCURACY within report interval: 0.41283783783783784\n",
      "Batch 380: LOSS within report interval: 1.5667771339416503 | ACCURACY within report interval: 0.41217105263157894\n",
      "Batch 390: LOSS within report interval: 1.5502398490905762 | ACCURACY within report interval: 0.413301282051282\n",
      "Batch 400: LOSS within report interval: 1.5936036348342895 | ACCURACY within report interval: 0.4140625\n",
      "Batch 410: LOSS within report interval: 1.4458716630935669 | ACCURACY within report interval: 0.41615853658536583\n",
      "Batch 420: LOSS within report interval: 1.3313514828681945 | ACCURACY within report interval: 0.41875\n",
      "Batch 430: LOSS within report interval: 1.4116684913635253 | ACCURACY within report interval: 0.4194767441860465\n",
      "Batch 440: LOSS within report interval: 1.442946183681488 | ACCURACY within report interval: 0.42102272727272727\n",
      "Batch 450: LOSS within report interval: 1.4272812008857727 | ACCURACY within report interval: 0.42319444444444443\n",
      "Batch 460: LOSS within report interval: 1.487856650352478 | ACCURACY within report interval: 0.42391304347826086\n",
      "Batch 470: LOSS within report interval: 1.4391899466514588 | ACCURACY within report interval: 0.4243351063829787\n",
      "Batch 480: LOSS within report interval: 1.389125609397888 | ACCURACY within report interval: 0.42669270833333334\n",
      "Batch 490: LOSS within report interval: 1.442112135887146 | ACCURACY within report interval: 0.42665816326530615\n",
      "Batch 500: LOSS within report interval: 1.4340962171554565 | ACCURACY within report interval: 0.427\n",
      "Batch 510: LOSS within report interval: 1.5222662687301636 | ACCURACY within report interval: 0.42769607843137253\n",
      "Batch 520: LOSS within report interval: 1.4332148551940918 | ACCURACY within report interval: 0.42788461538461536\n",
      "Batch 530: LOSS within report interval: 1.3724122881889342 | ACCURACY within report interval: 0.42948113207547167\n",
      "Batch 540: LOSS within report interval: 1.4882696628570558 | ACCURACY within report interval: 0.4287037037037037\n",
      "Batch 550: LOSS within report interval: 1.4726631879806518 | ACCURACY within report interval: 0.42863636363636365\n",
      "Batch 560: LOSS within report interval: 1.4209490299224854 | ACCURACY within report interval: 0.42935267857142856\n",
      "Batch 570: LOSS within report interval: 1.471617865562439 | ACCURACY within report interval: 0.42905701754385966\n",
      "Batch 580: LOSS within report interval: 1.3665682196617126 | ACCURACY within report interval: 0.43006465517241377\n",
      "Batch 590: LOSS within report interval: 1.4388548612594605 | ACCURACY within report interval: 0.4308262711864407\n",
      "Batch 600: LOSS within report interval: 1.2845256567001342 | ACCURACY within report interval: 0.433125\n",
      "Batch 610: LOSS within report interval: 1.4575802087783813 | ACCURACY within report interval: 0.4337090163934426\n",
      "Batch 620: LOSS within report interval: 1.4007829427719116 | ACCURACY within report interval: 0.43497983870967744\n",
      "Batch 630: LOSS within report interval: 1.4421224117279052 | ACCURACY within report interval: 0.4359126984126984\n",
      "Batch 640: LOSS within report interval: 1.3099602580070495 | ACCURACY within report interval: 0.43798828125\n",
      "Batch 650: LOSS within report interval: 1.550325608253479 | ACCURACY within report interval: 0.43788461538461537\n",
      "Batch 660: LOSS within report interval: 1.3694250583648682 | ACCURACY within report interval: 0.43892045454545453\n",
      "Batch 670: LOSS within report interval: 1.3570310592651367 | ACCURACY within report interval: 0.43871268656716417\n",
      "Batch 680: LOSS within report interval: 1.6061573505401612 | ACCURACY within report interval: 0.43851102941176473\n",
      "Batch 690: LOSS within report interval: 1.3945643901824951 | ACCURACY within report interval: 0.43876811594202897\n",
      "Batch 700: LOSS within report interval: 1.4323256134986877 | ACCURACY within report interval: 0.43892857142857145\n",
      "Evaluating...\n",
      "Evaluating...\n",
      "After Epoch 1: Train Accuracy: 0.49519914651493596 | Test Accuracy: 0.46088193456614507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60def9c1692240389f7eeee7d637b923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: LOSS within report interval: 1.2888161897659303 | ACCURACY within report interval: 0.50625\n",
      "Batch 20: LOSS within report interval: 1.3641527533531188 | ACCURACY within report interval: 0.525\n",
      "Batch 30: LOSS within report interval: 1.3695538759231567 | ACCURACY within report interval: 0.5041666666666667\n",
      "Batch 40: LOSS within report interval: 1.4748401999473573 | ACCURACY within report interval: 0.4921875\n",
      "Batch 50: LOSS within report interval: 1.3661333084106446 | ACCURACY within report interval: 0.5075\n",
      "Batch 60: LOSS within report interval: 1.3317946791648865 | ACCURACY within report interval: 0.509375\n",
      "Batch 70: LOSS within report interval: 1.397778046131134 | ACCURACY within report interval: 0.5098214285714285\n",
      "Batch 80: LOSS within report interval: 1.4318916916847229 | ACCURACY within report interval: 0.50390625\n",
      "Batch 90: LOSS within report interval: 1.4020453572273255 | ACCURACY within report interval: 0.5\n",
      "Batch 100: LOSS within report interval: 1.2933443069458008 | ACCURACY within report interval: 0.50375\n",
      "Batch 110: LOSS within report interval: 1.388953685760498 | ACCURACY within report interval: 0.5022727272727273\n",
      "Batch 120: LOSS within report interval: 1.3186094641685486 | ACCURACY within report interval: 0.503125\n",
      "Batch 130: LOSS within report interval: 1.4974192261695862 | ACCURACY within report interval: 0.4980769230769231\n",
      "Batch 140: LOSS within report interval: 1.4159096360206604 | ACCURACY within report interval: 0.4973214285714286\n",
      "Batch 150: LOSS within report interval: 1.3184466779232025 | ACCURACY within report interval: 0.49833333333333335\n",
      "Batch 160: LOSS within report interval: 1.5871020197868346 | ACCURACY within report interval: 0.49140625\n",
      "Batch 170: LOSS within report interval: 1.3367534399032592 | ACCURACY within report interval: 0.49080882352941174\n",
      "Batch 180: LOSS within report interval: 1.4689529418945313 | ACCURACY within report interval: 0.48854166666666665\n",
      "Batch 190: LOSS within report interval: 1.2802037715911865 | ACCURACY within report interval: 0.49078947368421055\n",
      "Batch 200: LOSS within report interval: 1.316377478837967 | ACCURACY within report interval: 0.491875\n",
      "Batch 210: LOSS within report interval: 1.361452692747116 | ACCURACY within report interval: 0.49375\n",
      "Batch 220: LOSS within report interval: 1.4109986543655395 | ACCURACY within report interval: 0.490625\n",
      "Batch 230: LOSS within report interval: 1.371910572052002 | ACCURACY within report interval: 0.4915760869565217\n",
      "Batch 240: LOSS within report interval: 1.2440555572509766 | ACCURACY within report interval: 0.49375\n",
      "Batch 250: LOSS within report interval: 1.3728996753692626 | ACCURACY within report interval: 0.49325\n",
      "Batch 260: LOSS within report interval: 1.386304771900177 | ACCURACY within report interval: 0.4935096153846154\n",
      "Batch 270: LOSS within report interval: 1.405551826953888 | ACCURACY within report interval: 0.49189814814814814\n",
      "Batch 280: LOSS within report interval: 1.4425707817077638 | ACCURACY within report interval: 0.4904017857142857\n",
      "Batch 290: LOSS within report interval: 1.423665714263916 | ACCURACY within report interval: 0.49051724137931035\n",
      "Batch 300: LOSS within report interval: 1.3535866498947144 | ACCURACY within report interval: 0.49125\n",
      "Batch 310: LOSS within report interval: 1.4189770936965942 | ACCURACY within report interval: 0.4913306451612903\n",
      "Batch 320: LOSS within report interval: 1.2240566253662108 | ACCURACY within report interval: 0.4935546875\n",
      "Batch 330: LOSS within report interval: 1.3180273473262787 | ACCURACY within report interval: 0.4933712121212121\n",
      "Batch 340: LOSS within report interval: 1.4484333395957947 | ACCURACY within report interval: 0.4920955882352941\n",
      "Batch 350: LOSS within report interval: 1.3141820907592774 | ACCURACY within report interval: 0.49357142857142855\n",
      "Batch 360: LOSS within report interval: 1.348823893070221 | ACCURACY within report interval: 0.49322916666666666\n",
      "Batch 370: LOSS within report interval: 1.3596646547317506 | ACCURACY within report interval: 0.4930743243243243\n",
      "Batch 380: LOSS within report interval: 1.3929598093032838 | ACCURACY within report interval: 0.4919407894736842\n",
      "Batch 390: LOSS within report interval: 1.3188128352165223 | ACCURACY within report interval: 0.49262820512820515\n",
      "Batch 400: LOSS within report interval: 1.4463058233261108 | ACCURACY within report interval: 0.49234375\n",
      "Batch 410: LOSS within report interval: 1.3400552153587342 | ACCURACY within report interval: 0.4929878048780488\n",
      "Batch 420: LOSS within report interval: 1.2864706695079804 | ACCURACY within report interval: 0.4936011904761905\n",
      "Batch 430: LOSS within report interval: 1.3593992829322814 | ACCURACY within report interval: 0.4943313953488372\n",
      "Batch 440: LOSS within report interval: 1.5322553873062135 | ACCURACY within report interval: 0.49232954545454544\n",
      "Batch 450: LOSS within report interval: 1.4880610227584838 | ACCURACY within report interval: 0.4911111111111111\n",
      "Batch 460: LOSS within report interval: 1.3157850623130798 | ACCURACY within report interval: 0.49184782608695654\n",
      "Batch 470: LOSS within report interval: 1.4068947792053224 | ACCURACY within report interval: 0.4908244680851064\n",
      "Batch 480: LOSS within report interval: 1.3377787947654725 | ACCURACY within report interval: 0.4908854166666667\n",
      "Batch 490: LOSS within report interval: 1.4559674620628358 | ACCURACY within report interval: 0.49081632653061225\n",
      "Batch 500: LOSS within report interval: 1.3103661060333252 | ACCURACY within report interval: 0.491375\n",
      "Batch 510: LOSS within report interval: 1.3732985138893128 | ACCURACY within report interval: 0.4906862745098039\n",
      "Batch 520: LOSS within report interval: 1.344618409872055 | ACCURACY within report interval: 0.490625\n",
      "Batch 530: LOSS within report interval: 1.346232682466507 | ACCURACY within report interval: 0.49139150943396226\n",
      "Batch 540: LOSS within report interval: 1.3421729683876038 | ACCURACY within report interval: 0.4914351851851852\n",
      "Batch 550: LOSS within report interval: 1.3737681031227111 | ACCURACY within report interval: 0.4913636363636364\n",
      "Batch 560: LOSS within report interval: 1.2864079654216767 | ACCURACY within report interval: 0.49174107142857143\n",
      "Batch 570: LOSS within report interval: 1.4054211735725404 | ACCURACY within report interval: 0.49199561403508774\n",
      "Batch 580: LOSS within report interval: 1.3758766293525695 | ACCURACY within report interval: 0.49181034482758623\n",
      "Batch 590: LOSS within report interval: 1.3886277794837951 | ACCURACY within report interval: 0.4912076271186441\n",
      "Batch 600: LOSS within report interval: 1.3468208611011505 | ACCURACY within report interval: 0.49052083333333335\n",
      "Batch 610: LOSS within report interval: 1.4752435564994812 | ACCURACY within report interval: 0.4900614754098361\n",
      "Batch 620: LOSS within report interval: 1.3980839490890502 | ACCURACY within report interval: 0.4899193548387097\n",
      "Batch 630: LOSS within report interval: 1.3009029388427735 | ACCURACY within report interval: 0.4901785714285714\n",
      "Batch 640: LOSS within report interval: 1.3041163563728333 | ACCURACY within report interval: 0.49091796875\n",
      "Batch 650: LOSS within report interval: 1.460974431037903 | ACCURACY within report interval: 0.4901923076923077\n",
      "Batch 660: LOSS within report interval: 1.4787866473197937 | ACCURACY within report interval: 0.4895833333333333\n",
      "Batch 670: LOSS within report interval: 1.4171775460243226 | ACCURACY within report interval: 0.48880597014925375\n",
      "Batch 680: LOSS within report interval: 1.3475152254104614 | ACCURACY within report interval: 0.4892463235294118\n",
      "Batch 690: LOSS within report interval: 1.3340583324432373 | ACCURACY within report interval: 0.4890398550724638\n",
      "Batch 700: LOSS within report interval: 1.3832449436187744 | ACCURACY within report interval: 0.48910714285714285\n",
      "Evaluating...\n",
      "Evaluating...\n",
      "After Epoch 2: Train Accuracy: 0.5202702702702703 | Test Accuracy: 0.4448790896159317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092278403bc44e538cd166e4e7a8dfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: LOSS within report interval: 1.412113881111145 | ACCURACY within report interval: 0.5\n",
      "Batch 20: LOSS within report interval: 1.4006840705871582 | ACCURACY within report interval: 0.46875\n",
      "Batch 30: LOSS within report interval: 1.3991490602493286 | ACCURACY within report interval: 0.47708333333333336\n",
      "Batch 40: LOSS within report interval: 1.2616963028907775 | ACCURACY within report interval: 0.5\n",
      "Batch 50: LOSS within report interval: 1.3619627714157105 | ACCURACY within report interval: 0.50125\n",
      "Batch 60: LOSS within report interval: 1.436678123474121 | ACCURACY within report interval: 0.49895833333333334\n",
      "Batch 70: LOSS within report interval: 1.4003711223602295 | ACCURACY within report interval: 0.4901785714285714\n",
      "Batch 80: LOSS within report interval: 1.3139843463897705 | ACCURACY within report interval: 0.4984375\n",
      "Batch 90: LOSS within report interval: 1.517417049407959 | ACCURACY within report interval: 0.4888888888888889\n",
      "Batch 100: LOSS within report interval: 1.4612109303474425 | ACCURACY within report interval: 0.48125\n",
      "Batch 110: LOSS within report interval: 1.4619632720947267 | ACCURACY within report interval: 0.4784090909090909\n",
      "Batch 120: LOSS within report interval: 1.3601982235908507 | ACCURACY within report interval: 0.4765625\n",
      "Batch 130: LOSS within report interval: 1.2648167252540587 | ACCURACY within report interval: 0.48125\n",
      "Batch 140: LOSS within report interval: 1.302701711654663 | ACCURACY within report interval: 0.484375\n",
      "Batch 150: LOSS within report interval: 1.3862704157829284 | ACCURACY within report interval: 0.48333333333333334\n",
      "Batch 160: LOSS within report interval: 1.3629096508026124 | ACCURACY within report interval: 0.483203125\n",
      "Batch 170: LOSS within report interval: 1.2736553013324738 | ACCURACY within report interval: 0.48676470588235293\n",
      "Batch 180: LOSS within report interval: 1.3301745414733888 | ACCURACY within report interval: 0.48854166666666665\n",
      "Batch 190: LOSS within report interval: 1.4033612251281737 | ACCURACY within report interval: 0.48651315789473687\n",
      "Batch 200: LOSS within report interval: 1.3791236519813537 | ACCURACY within report interval: 0.4846875\n",
      "Batch 210: LOSS within report interval: 1.2915289521217346 | ACCURACY within report interval: 0.48779761904761904\n",
      "Batch 220: LOSS within report interval: 1.3234787464141846 | ACCURACY within report interval: 0.48863636363636365\n",
      "Batch 230: LOSS within report interval: 1.4002071261405944 | ACCURACY within report interval: 0.48695652173913045\n",
      "Batch 240: LOSS within report interval: 1.2845588088035584 | ACCURACY within report interval: 0.48932291666666666\n",
      "Batch 250: LOSS within report interval: 1.456106722354889 | ACCURACY within report interval: 0.48675\n",
      "Batch 260: LOSS within report interval: 1.2788296937942505 | ACCURACY within report interval: 0.4891826923076923\n",
      "Batch 270: LOSS within report interval: 1.4394216060638427 | ACCURACY within report interval: 0.48726851851851855\n",
      "Batch 280: LOSS within report interval: 1.275847601890564 | ACCURACY within report interval: 0.48973214285714284\n",
      "Batch 290: LOSS within report interval: 1.2847464203834533 | ACCURACY within report interval: 0.4898706896551724\n",
      "Batch 300: LOSS within report interval: 1.3100636959075929 | ACCURACY within report interval: 0.4897916666666667\n",
      "Batch 310: LOSS within report interval: 1.3529707789421082 | ACCURACY within report interval: 0.4905241935483871\n",
      "Batch 320: LOSS within report interval: 1.3802292227745057 | ACCURACY within report interval: 0.4896484375\n",
      "Batch 330: LOSS within report interval: 1.247875154018402 | ACCURACY within report interval: 0.49015151515151517\n",
      "Batch 340: LOSS within report interval: 1.3468368172645568 | ACCURACY within report interval: 0.49136029411764703\n",
      "Batch 350: LOSS within report interval: 1.295543134212494 | ACCURACY within report interval: 0.49107142857142855\n",
      "Batch 360: LOSS within report interval: 1.3044010162353517 | ACCURACY within report interval: 0.4918402777777778\n",
      "Batch 370: LOSS within report interval: 1.4464049220085144 | ACCURACY within report interval: 0.4922297297297297\n",
      "Batch 380: LOSS within report interval: 1.3688099563121796 | ACCURACY within report interval: 0.4921052631578947\n",
      "Batch 390: LOSS within report interval: 1.3283758163452148 | ACCURACY within report interval: 0.492948717948718\n",
      "Batch 400: LOSS within report interval: 1.222831952571869 | ACCURACY within report interval: 0.494375\n",
      "Batch 410: LOSS within report interval: 1.2154992341995239 | ACCURACY within report interval: 0.49527439024390246\n",
      "Batch 420: LOSS within report interval: 1.423701548576355 | ACCURACY within report interval: 0.49523809523809526\n",
      "Batch 430: LOSS within report interval: 1.2897945165634155 | ACCURACY within report interval: 0.4965116279069767\n",
      "Batch 440: LOSS within report interval: 1.288909387588501 | ACCURACY within report interval: 0.4967329545454545\n",
      "Batch 450: LOSS within report interval: 1.2813900709152222 | ACCURACY within report interval: 0.4970833333333333\n",
      "Batch 460: LOSS within report interval: 1.1799588322639465 | ACCURACY within report interval: 0.4983695652173913\n",
      "Batch 470: LOSS within report interval: 1.3256373465061189 | ACCURACY within report interval: 0.49867021276595747\n",
      "Batch 480: LOSS within report interval: 1.354115867614746 | ACCURACY within report interval: 0.49856770833333336\n",
      "Batch 490: LOSS within report interval: 1.2939541339874268 | ACCURACY within report interval: 0.49923469387755104\n",
      "Batch 500: LOSS within report interval: 1.259956383705139 | ACCURACY within report interval: 0.499875\n",
      "Batch 510: LOSS within report interval: 1.3627992868423462 | ACCURACY within report interval: 0.4991421568627451\n",
      "Batch 520: LOSS within report interval: 1.2280597805976867 | ACCURACY within report interval: 0.5009615384615385\n",
      "Batch 530: LOSS within report interval: 1.4007619738578796 | ACCURACY within report interval: 0.5007075471698114\n",
      "Batch 540: LOSS within report interval: 1.3146579384803772 | ACCURACY within report interval: 0.5002314814814814\n",
      "Batch 550: LOSS within report interval: 1.3062336206436158 | ACCURACY within report interval: 0.5\n",
      "Batch 560: LOSS within report interval: 1.3113351702690124 | ACCURACY within report interval: 0.4994419642857143\n",
      "Batch 570: LOSS within report interval: 1.35600745677948 | ACCURACY within report interval: 0.4990131578947368\n",
      "Batch 580: LOSS within report interval: 1.2972758769989015 | ACCURACY within report interval: 0.4984913793103448\n",
      "Batch 590: LOSS within report interval: 1.3809424042701721 | ACCURACY within report interval: 0.49915254237288137\n",
      "Batch 600: LOSS within report interval: 1.49239581823349 | ACCURACY within report interval: 0.4979166666666667\n",
      "Batch 610: LOSS within report interval: 1.3978697061538696 | ACCURACY within report interval: 0.49774590163934423\n",
      "Batch 620: LOSS within report interval: 1.2891543984413147 | ACCURACY within report interval: 0.4981854838709677\n",
      "Batch 630: LOSS within report interval: 1.33555269241333 | ACCURACY within report interval: 0.4978174603174603\n",
      "Batch 640: LOSS within report interval: 1.1559482932090759 | ACCURACY within report interval: 0.49873046875\n",
      "Batch 650: LOSS within report interval: 1.2703458428382874 | ACCURACY within report interval: 0.49923076923076926\n",
      "Batch 660: LOSS within report interval: 1.248070526123047 | ACCURACY within report interval: 0.49952651515151514\n",
      "Batch 670: LOSS within report interval: 1.3723487615585328 | ACCURACY within report interval: 0.49934701492537314\n",
      "Batch 680: LOSS within report interval: 1.334787529706955 | ACCURACY within report interval: 0.5007352941176471\n",
      "Batch 690: LOSS within report interval: 1.2936195969581603 | ACCURACY within report interval: 0.5007246376811594\n",
      "Batch 700: LOSS within report interval: 1.3501559615135192 | ACCURACY within report interval: 0.5010714285714286\n",
      "Evaluating...\n",
      "Evaluating...\n",
      "After Epoch 3: Train Accuracy: 0.5215149359886202 | Test Accuracy: 0.4544807965860597\n"
     ]
    }
   ],
   "source": [
    "model = setup_model(len(id2label), frozen_backbone=True)\n",
    "\n",
    "\n",
    "# Define Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "EPOCHS = 3\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "save_dir = f\"{DATA_PATH}/Models/Assessor/ResNet/\"\n",
    "train_model(model, train_dataset, test_dataset, criterion, optimizer, EPOCHS, BATCH_SIZE, report_interval=10, eval_every=1000000, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{save_dir}model_epoch_3.pt\"\n",
    "model = torch.load(save_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58c09b33c5a43298e2ab87b9b84dd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sku \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msku\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Zalando_Germany_Dataset/dresses/images/square_images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msku\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtest_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m label \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/torchvision/transforms/functional.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    465\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    466\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/PIL/Image.py:2163\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2161\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[0;32m-> 2163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2165\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/master_thesis/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "for i in tqdm(df.index, total=len(df.index)):\n",
    "    sku = df.loc[i]['sku']\n",
    "    img = Image.open(f\"{DATA_PATH}/Zalando_Germany_Dataset/dresses/images/square_images/{sku}.jpg\")\n",
    "    img = test_transform(img).unsqueeze(0).to(device)\n",
    "    label = df.loc[i]['label']\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        prob, predicted = torch.max(softmax(output.data, 1), 1)\n",
    "        df.loc[i, 'predicted_label'] = predicted.item()\n",
    "        df.loc[i, 'predicted_prob'] = prob.item()\n",
    "\n",
    "df['predicted_category'] = df.predicted_label.map(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = evaluate(model, DataLoader(train_dataset, batch_size=32, shuffle=False))\n",
    "test_acc = evaluate(model, DataLoader(test_dataset, batch_size=32, shuffle=False))\n",
    "print(f\"Train Accuracy: {np.round(train_acc*100, 2)}%, Test Accuracy: {np.round(test_acc*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "overall_accuracy = accuracy_score(df['label'], df['predicted_label'])\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy for a group\n",
    "def group_accuracy(group):\n",
    "    return accuracy_score(group['label'], group['predicted_label'])\n",
    "\n",
    "# Calculate accuracy for each category\n",
    "group_accuracy = df.groupby(target_feature).apply(group_accuracy).sort_values(ascending=False)\n",
    "\n",
    "# Print the accuracy for each category\n",
    "print(\"Accuracy by Group:\")\n",
    "print(group_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(group_accuracy).reset_index().rename(columns={0: 'accuracy'}).merge(df[target_feature].value_counts().reset_index())\n",
    "# Plot accuracy against number of samples and name each dot\n",
    "plt.figure()\n",
    "import seaborn as sns\n",
    "sns.scatterplot(data=plot_data, x='accuracy', y='count')\n",
    "for i in range(plot_data.shape[0]):\n",
    "    plt.text(plot_data.accuracy[i], plot_data['count'][i], plot_data[target_feature][i], fontsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "confusion_matrix = pd.crosstab(df['predicted_category'], df[target_feature], rownames=['Predicted'], colnames=['Actual'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see some misclassified examples\n",
    "root_path = f\"{DATA_PATH}/Zalando_Germany_Dataset/dresses/images/square_images/\"\n",
    "misclassified = df[df['label'] != df['predicted_label']]\n",
    "misclassified = misclassified.sample(1)\n",
    "for i, row in misclassified.iterrows():\n",
    "    img_path = f\"{root_path}/{row['sku']}.jpg\"\n",
    "    img = Image.open(img_path)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Actual: {row[target_feature]}\\nPredicted: {row['predicted_category']}\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, train_loader, test_loader, criterion, optimizer, epoch_num, report_interval=2):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "#     running_total = 0\n",
    "\n",
    "#     # Configure how often to evaluate based on the total images shown\n",
    "#     eval_steps = max(1, 1000 // train_loader.batch_size)  \n",
    "\n",
    "#     for i, data in enumerate(train_loader):\n",
    "#         images, labels = data\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         running_corrects += (predicted == labels).sum().item()\n",
    "#         running_total += labels.size(0)\n",
    "\n",
    "#         if i % eval_steps == eval_steps-1:\n",
    "#             test_acc = evaluate(model, test_loader)\n",
    "#             wandb.log({'test_accuracy': test_acc})\n",
    "#             print(f\"Batch {i+1}: Running LOSS: {running_loss / eval_steps} | Running ACCURACY: {running_corrects / running_total} | TEST ACCURACY: {test_acc}\")\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "#             running_total = 0\n",
    "\n",
    "#     train_acc = evaluate(model, train_loader)\n",
    "#     wandb.log({'train_accuracy': train_acc, 'final_test_accuracy': evaluate(model, test_loader)})\n",
    "\n",
    "# def evaluate(model, data_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in data_loader:\n",
    "#             images, labels = data\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#     return correct / total\n",
    "\n",
    "# def train_model(config=None):\n",
    "#     with wandb.init(config=config):\n",
    "#         config = wandb.config\n",
    "\n",
    "#         # Define your model, criterion, and optimizer here\n",
    "#         model = setup_model(len(id2label))\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = get_optimizer(model, config.optimizer, config.learning_rate)\n",
    "        \n",
    "#         train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "#         for epoch in range(config.epochs):\n",
    "#             train_epoch(model, train_loader, test_loader, criterion, optimizer, epoch + 1)\n",
    "\n",
    "# def get_optimizer(model, optimizer_name, learning_rate):\n",
    "#     if optimizer_name == 'adam':\n",
    "#         return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     elif optimizer_name == 'sgd':\n",
    "#         return torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='model_finetuning')\n",
    "\n",
    "# # Sweep configuration\n",
    "# sweep_config = {\n",
    "#     'method': 'random',\n",
    "#     'metric': {\n",
    "#         'name': 'final_test_accuracy',\n",
    "#         'goal': 'maximize'\n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         'batch_size': {\n",
    "#             'values': [8, 16, 32, 64]\n",
    "#         },\n",
    "#         'learning_rate': {\n",
    "#             'values': [0.001, 0.0001, 0.01]\n",
    "#         },\n",
    "#         'optimizer': {\n",
    "#             'values': ['adam', 'sgd']\n",
    "#         },\n",
    "#         'epochs': {\n",
    "#             'value':  1 # Adjust number of epochs for each run\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project='resnet_category_tuning')\n",
    "# wandb.agent(sweep_id, train_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
