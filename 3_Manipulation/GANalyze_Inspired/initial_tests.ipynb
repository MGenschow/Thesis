{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == 'Darwin':\n",
    "    DATA_PATH = \"/Users/maltegenschow/Documents/Uni/Thesis/Data.nosync\"\n",
    "    ROOT_PATH = \"/Users/maltegenschow/Documents/Uni/Thesis/Thesis\"\n",
    "elif platform.system() == 'Linux':\n",
    "    DATA_PATH = \"/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Data.nosync\"\n",
    "    ROOT_PATH = \"/pfs/work7/workspace/scratch/tu_zxmav84-thesis/Thesis\"\n",
    "\n",
    "current_wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "os.chdir(f\"{ROOT_PATH}/4_Assessor/Category_Assessor/DinoV2\")\n",
    "from helpers_pipeline import *\n",
    "id2label = pickle.load(open(\"id2label_dicts/category_id2label.pkl\", \"rb\"))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "os.chdir(current_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Models Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Codes for all images\n",
    "target_feature = 'category'\n",
    "df, latents = load_latents(target_feature)\n",
    "# Generator\n",
    "G = setup_generator()\n",
    "#G = G.to(device)\n",
    "# DinoV2 Backbone\n",
    "dino_processor_old, dino_model = setup_dinov2()\n",
    "# Trained Classifier model \n",
    "classifier = load_classifier() \n",
    "#classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeeze all non-relevant model weights and set to eval mode\n",
    "for param in G.parameters():\n",
    "    param.requires_grad = False\n",
    "G.eval()\n",
    "for param in dino_model.parameters():\n",
    "    param.requires_grad = False\n",
    "dino_model.eval()\n",
    "for param in classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "classifier.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: mps:0 | Requires Grad: False\n",
      "DinoV2: cpu | Requires Grad: False\n",
      "Classifier: mps:0 | Requires Grad: False\n"
     ]
    }
   ],
   "source": [
    "# Print devoce for each model: \n",
    "print(f\"Generator: {next(G.parameters()).device} | Requires Grad: {next(G.parameters()).requires_grad}\")\n",
    "print(f\"DinoV2: {next(dino_model.parameters()).device} | Requires Grad: {next(dino_model.parameters()).requires_grad}\")\n",
    "print(f\"Classifier: {next(classifier.parameters()).device} | Requires Grad: {next(classifier.parameters()).requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Classifier Function to be only Tensors and differentiable functions\n",
    "-> Most important function to change: processor function of DinoV2 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    #transforms.Resize(256),  # Resize so the shortest side is 256\n",
    "    #transforms.CenterCrop((224, 224)),  # Center crop to 224x224\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_img = G.synthesis(latents[0], noise_mode='const')\n",
    "gen_img = gen_img.to('cpu')\n",
    "gen_processed = transform_pipeline(gen_img)\n",
    "gen_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('/Users/maltegenschow/Documents/Uni/Thesis/Data.nosync/Zalando_Germany_Dataset/dresses/images/square_images/0FB21C03E-I11.jpg').convert('RGB')\n",
    "img = transforms.ToTensor()(img.resize([512,512]))\n",
    "img = img.unsqueeze(0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dino_processor(input):\n",
    "    if isinstance(input, str):\n",
    "        img = Image.open(input).convert('RGB')\n",
    "        img = transforms.ToTensor()(img.resize([512,512]))\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        processed_img = transform_pipeline(img)\n",
    "    elif isinstance(input, torch.Tensor):\n",
    "        processed_img = transform_pipeline(input)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a string or a torch.Tensor\")\n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the gradients flow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latents[0]\n",
    "directions = torch.randn([8, 16,512], device=device, requires_grad=True)\n",
    "\n",
    "target = torch.tensor(1, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.21093101799488068 | Directions Sum: 181.53134155273438\n",
      "Loss: 0.12418025732040405 | Directions Sum: 181.4152069091797\n",
      "Loss: 0.11543690413236618 | Directions Sum: 183.40005493164062\n",
      "Loss: 0.10273753106594086 | Directions Sum: 185.85565185546875\n",
      "Loss: 0.0880921259522438 | Directions Sum: 188.21725463867188\n",
      "Loss: 0.0784483253955841 | Directions Sum: 190.4754638671875\n",
      "Loss: 0.07277145981788635 | Directions Sum: 192.72296142578125\n",
      "Loss: 0.06608559936285019 | Directions Sum: 194.8636932373047\n",
      "Loss: 0.06189819797873497 | Directions Sum: 196.83029174804688\n",
      "Loss: 0.06023789942264557 | Directions Sum: 198.9141082763672\n",
      "Loss: 0.05609115585684776 | Directions Sum: 200.9251708984375\n",
      "Loss: 0.053432293236255646 | Directions Sum: 202.85855102539062\n",
      "Loss: 0.049008943140506744 | Directions Sum: 204.70999145507812\n",
      "Loss: 0.04497193545103073 | Directions Sum: 206.62374877929688\n",
      "Loss: 0.042290929704904556 | Directions Sum: 208.54299926757812\n",
      "Loss: 0.03941873461008072 | Directions Sum: 210.34414672851562\n",
      "Loss: 0.03668229654431343 | Directions Sum: 211.8153533935547\n",
      "Loss: 0.033965304493904114 | Directions Sum: 212.9066162109375\n",
      "Loss: 0.032086025923490524 | Directions Sum: 213.67825317382812\n",
      "Loss: 0.030317196622490883 | Directions Sum: 214.17190551757812\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([directions], lr=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    transformed = latent + 0.1 * directions[0]\n",
    "    gen_img = G.synthesis(transformed, noise_mode='const')\n",
    "    gen_img = gen_img.to('cpu')\n",
    "    gen_processed = dino_processor(gen_img)\n",
    "    embedding = dino_model(gen_processed)['pooler_output']\n",
    "    embedding = embedding.to(device)\n",
    "    scores = classifier(embedding)\n",
    "    scores = softmax(scores, dim=1)\n",
    "    scores = scores[0][0]\n",
    "\n",
    "    \n",
    "\n",
    "    loss = criterion(scores, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss: {loss.item()} | Directions Sum: {directions.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, generator, dino_model, dino_processor, classifier, get_attribute_scores, id2label, label2id, ):\n",
    "        super(Editor, self).__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.dino_model = dino_model\n",
    "        self.dino_processor = dino_processor\n",
    "        self.classifier = classifier\n",
    "        self.get_attribute_score = get_attribute_scores\n",
    "\n",
    "        self.id2label = id2label\n",
    "        self.label2id = label2id   \n",
    "        self.num_classes = len(self.id2label)\n",
    "        self.directions_dimension = [generator.mapping.num_ws, generator.mapping.w_dim]\n",
    "\n",
    "        self.directions = nn.Parameter(torch.randn(self.num_classes,self.directions_dimension[0], self.directions_dimension[1]), requires_grad=True)\n",
    "        self.alphas = np.arange(0,1,0.1)\n",
    "    \n",
    "    def gan_output_to_image(self, output):\n",
    "        img_perm = (output.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "        img = Image.fromarray(img_perm[0].cpu().numpy(), 'RGB')\n",
    "        return img\n",
    "\n",
    "\n",
    "    def forward(self, latents, class_idx=None, alpha=None):\n",
    "\n",
    "        if class_idx == None:\n",
    "            class_idx = torch.randint(0, self.num_classes, (1,))\n",
    "        \n",
    "        if alpha == None:\n",
    "            alpha = torch.tensor(np.round(np.random.choice(self.alphas),2))\n",
    "\n",
    "        # Get scores for original image\n",
    "        real_img = self.generator.synthesis(latents, noise_mode='const')\n",
    "        real_img = self.gan_output_to_image(real_img)\n",
    "        real_scores = self.get_attribute_score(self.dino_model, self.dino_processor, self.classifier, real_img)\n",
    "        real_probs = softmax(real_scores, dim=0)\n",
    "        real_class_prob  = real_probs[class_idx]\n",
    "\n",
    "        # Get scores for transformed image\n",
    "        transformed_latent = latents.clone() + alpha * self.directions[class_idx].to(latents.device)\n",
    "        transformed_img = self.generator.synthesis(transformed_latent, noise_mode='const')\n",
    "        transformed_img = self.gan_output_to_image(transformed_img)\n",
    "        transformed_scores = self.get_attribute_score(self.dino_model, self.dino_processor, self.classifier, transformed_img)\n",
    "        transformed_probs = softmax(transformed_scores, dim=0)\n",
    "        transformed_class_prob = transformed_probs[class_idx]\n",
    "\n",
    "        real_class_prob.requires_grad = True\n",
    "        transformed_class_prob.requires_grad = True\n",
    "\n",
    "        return real_class_prob, transformed_class_prob, class_idx, alpha\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Editor(G, dino_model, dino_processor, classifier, get_attribute_scores, id2label, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on one example only\n",
    "\n",
    "- First latent code in the dataset\n",
    "- Real Label: Day Dress: class_idx = 0\n",
    "- Target Label: Denim Dress: class_idx = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_alpha = 0.1\n",
    "fixed_class_idx = 7\n",
    "\n",
    "model = Editor(G, dino_model, dino_processor, classifier, get_attribute_scores, id2label, label2id) \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5)\n",
    "\n",
    "for i in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward:\n",
    "    real_class_prob, transformed_class_prob, class_idx, alpha = model(latents[0], 7, 0.1)\n",
    "    # Loss:\n",
    "    loss = criterion(transformed_class_prob, (real_class_prob + fixed_alpha))\n",
    "    # Backward:\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print updates: \n",
    "    print(f\"Step: {i} | Loss: {np.round(loss.item(),4)} | Real Class Prob: {real_class_prob.item()} | Transformed Class Prob: {transformed_class_prob.item()}\")\n",
    "    print(f\"Sum of walking direction: {torch.sum(model.directions[class_idx])} | Alpha: {alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `latents` is a tensor with requires_grad=True\n",
    "latent = latents[0]\n",
    "generated_image = model.generator.synthesis(latent, noise_mode='const')\n",
    "# Simulate the operation\n",
    "out = model.gan_output_to_image(generated_image)\n",
    "\n",
    "# Perform a dummy operation and check gradients\n",
    "output = pil_to_tensor.sum()\n",
    "output.backward()\n",
    "\n",
    "print(\"Gradient to latents after image conversion:\", latents.grad)  # This will likely show None or zero\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
